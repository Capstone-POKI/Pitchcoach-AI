"""
Document AI API í˜¸ì¶œ + ê°•í™” ê¸°ëŠ¥
"""

import json
import re
import os
from typing import Dict, List
from google.cloud import documentai_v1beta3 as documentai

from src.utils.io_utils import save_json, read_bytes
from src.utils.pdf_split import split_pdf

PROJECT_ID = os.getenv("PROJECT_ID", "pitchcoachai")
LOCATION = os.getenv("LOCATION", "us")
PROCESSORS = {
    "OCR": os.getenv("OCR_PROCESSOR_ID", "e41bb5d1cae96184"),
    "LAYOUT": os.getenv("LAYOUT_PROCESSOR_ID", "82698693210d7aa8"),
    "FORM": os.getenv("FORM_PROCESSOR_ID", "662d7f1f1e179648"),
}


# ì„¹ì…˜ ê°ì§€ íŒ¨í„´
SECTION_KEYWORDS = {
    "cover": ["ê²½ì§„ëŒ€íšŒ", "pitch deck", "ir deck", "ë°œí‘œìë£Œ"],
    "background": ["background", "ë°°ê²½", "í˜„í™©", "ë¬¸ì œì œê¸°", "ì‹œì¥ ë°°ê²½"],
    "problem": ["problem", "ë¬¸ì œì ", "pain point", "í•´ê²°í•˜ê³ ì", "ë¶ˆí¸í•¨"],
    "solution": ["solution", "ì†”ë£¨ì…˜", "í•´ê²°ë°©ì•ˆ", "ìš°ë¦¬ì˜ ë‹µ"],
    "product": ["product", "ì œí’ˆ", "ì„œë¹„ìŠ¤", "í•µì‹¬ ê¸°ëŠ¥"],
    "market": ["market", "ì‹œì¥", "tam", "sam", "som", "ì‹œì¥ ê·œëª¨", "íŠ¸ë Œë“œ"],
    "competition": ["competition", "ê²½ìŸ", "competitive", "ì°¨ë³„ì ", "ê²½ìŸìš°ìœ„"],
    "business_model": ["business model", "ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸", "ìˆ˜ìµ ëª¨ë¸", "revenue"],
    "finance": ["finance", "ì¬ë¬´", "ë§¤ì¶œ", "íˆ¬ì", "unit economics"],
    "team": ["team", "íŒ€", "êµ¬ì„±ì›", "ê²½ë ¥", "ë©¤ë²„"],
    "growth": ["growth", "ì„±ì¥", "í™•ì¥", "ê³„íš", "roadmap", "milestone"],
}


# ìˆ«ì ì¶”ì¶œ íŒ¨í„´
NUMBER_PATTERNS = {
    "currency_korean": [
        r"(\d+(?:,\d{3})*(?:\.\d+)?)\s*ì–µ\s*ì›?",
        r"(\d+(?:,\d{3})*(?:\.\d+)?)\s*ì¡°\s*ì›?",
        r"(\d+(?:,\d{3})*(?:\.\d+)?)\s*ë§Œ\s*ì›?",
    ],
    "percentage": [
        r"(\d+(?:\.\d+)?)\s*%",
    ],
    "quantity": [
        r"(\d+(?:,\d{3})*)\s*ì–µ?\s*ê°œ",
        r"(\d+(?:,\d{3})*)\s*ëŒ€",
        r"(\d+(?:,\d{3})*)\s*ëª…",
    ],
}


def process_document(
    file_path: str,
    processor_type: str,
    output_path: str,
    enable_enhancement: bool = True
) -> Dict:
    """Document AI API í˜¸ì¶œ + ê°•í™” ê¸°ëŠ¥"""
    
    processor_id = PROCESSORS[processor_type]
    
    client = documentai.DocumentProcessorServiceClient()
    name = client.processor_path(PROJECT_ID, LOCATION, processor_id)
    
    print(f"ğŸ“„ [{processor_type}] {file_path} ë¶„ì„ ì‹œì‘...")
    
    # ê¸°ì¡´ ìœ í‹¸ ì‚¬ìš©
    content = read_bytes(file_path)
    
    raw_document = documentai.RawDocument(
        content=content,
        mime_type="application/pdf",
    )
    
    # OCR ì˜µì…˜ ê°•í™”
    if processor_type == "OCR":
        process_options = documentai.ProcessOptions(
            ocr_config=documentai.OcrConfig(
                compute_style_info=True,
                enable_native_pdf_parsing=True,
                enable_image_quality_scores=True,
                enable_symbol=True,
            )
        )
        request = documentai.ProcessRequest(
            name=name,
            raw_document=raw_document,
            process_options=process_options
        )
    else:
        request = documentai.ProcessRequest(
            name=name,
            raw_document=raw_document
        )
    
    result = client.process_document(request=request)
    doc = result.document
    
    # Document AI Document â†’ dict
    doc_dict = json.loads(documentai.Document.to_json(doc))
    
    # ê°•í™” ê¸°ëŠ¥ ì ìš©
    if enable_enhancement:
        print(f"ğŸ”§ ê°•í™” ê¸°ëŠ¥ ì ìš© ì¤‘...")
        doc_dict = detect_sections(doc_dict)
        doc_dict = extract_numbers(doc_dict)
        doc_dict = generate_metadata(doc_dict)
        
        print(f"âœ… ê°•í™” ì™„ë£Œ: {len(doc_dict.get('detected_sections', []))}ê°œ ì„¹ì…˜, "
              f"{sum(len(v) for v in doc_dict.get('extracted_numbers', {}).values())}ê°œ ìˆ«ì ì¶”ì¶œ")
    
    # ê¸°ì¡´ ìœ í‹¸ ì‚¬ìš©
    save_json(doc_dict, output_path)
    print(f"âœ… [{processor_type}] ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {output_path}\n")
    
    return doc_dict


def process_pdf_ocr_in_chunks(
    file_path: str,
    output_dir: str,
    pages_per_chunk: int = 15,
    enable_enhancement: bool = True
) -> List[Dict]:
    """ëŒ€ìš©ëŸ‰ PDFë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ OCR ì²˜ë¦¬"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\nğŸ“„ ëŒ€ìš©ëŸ‰ PDF ì²­í¬ ì²˜ë¦¬: {file_path}")
    print(f"  - ì²­í¬ í¬ê¸°: {pages_per_chunk}í˜ì´ì§€")
    print(f"  - ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    # ê¸°ì¡´ pdf_split ì‚¬ìš©
    chunk_files = split_pdf(file_path, output_dir, pages_per_chunk)
    
    print(f"  âœ… {len(chunk_files)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ\n")
    
    results = []
    
    for idx, chunk_path in enumerate(chunk_files, 1):
        print(f"ğŸ“„ ì²­í¬ {idx}/{len(chunk_files)} ì²˜ë¦¬ ì¤‘...")
        
        chunk_name = os.path.splitext(os.path.basename(chunk_path))[0]
        output_path = os.path.join(output_dir, f"{chunk_name}_ocr.json")
        
        result = process_document(
            file_path=chunk_path,
            processor_type="OCR",
            output_path=output_path,
            enable_enhancement=enable_enhancement
        )
        
        result["chunk_info"] = {
            "chunk_index": idx,
            "total_chunks": len(chunk_files),
            "chunk_file": chunk_path,
        }
        
        results.append(result)
    
    print(f"\nâœ… ì „ì²´ {len(results)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ\n")
    
    return results


def detect_sections(doc_dict: Dict) -> Dict:
    """í˜ì´ì§€ë³„ ì„¹ì…˜ ìë™ ê°ì§€"""
    
    pages = doc_dict.get("pages", [])
    full_text = doc_dict.get("text", "")
    detected_sections = []
    
    for page_idx, page in enumerate(pages):
        blocks = page.get("blocks", [])
        if not blocks:
            continue
        
        first_block = blocks[0]
        block_text = _extract_block_text(first_block, full_text).lower()
        
        section_type = "unknown"
        for section, keywords in SECTION_KEYWORDS.items():
            if any(keyword.lower() in block_text for keyword in keywords):
                section_type = section
                break
        
        detected_sections.append({
            "page": page_idx + 1,
            "section": section_type,
            "preview": block_text[:100]
        })
        
        page["detected_section"] = section_type
    
    doc_dict["detected_sections"] = detected_sections
    return doc_dict


def extract_numbers(doc_dict: Dict) -> Dict:
    """ìˆ«ì/í†µê³„ ë°ì´í„° ìë™ ì¶”ì¶œ"""
    
    full_text = doc_dict.get("text", "")
    extracted = {
        "currency": [],
        "percentage": [],
        "quantity": [],
    }
    
    # í™”í ì¶”ì¶œ
    for pattern in NUMBER_PATTERNS["currency_korean"]:
        for match in re.finditer(pattern, full_text):
            extracted["currency"].append({
                "text": match.group(0),
                "value": match.group(1),
                "position": match.start()
            })
    
    # ë°±ë¶„ìœ¨ ì¶”ì¶œ
    for pattern in NUMBER_PATTERNS["percentage"]:
        for match in re.finditer(pattern, full_text):
            extracted["percentage"].append({
                "text": match.group(0),
                "value": match.group(1),
                "position": match.start()
            })
    
    # ìˆ˜ëŸ‰ ì¶”ì¶œ
    for pattern in NUMBER_PATTERNS["quantity"]:
        for match in re.finditer(pattern, full_text):
            extracted["quantity"].append({
                "text": match.group(0),
                "value": match.group(1).replace(",", ""),
                "position": match.start()
            })
    
    doc_dict["extracted_numbers"] = extracted
    return doc_dict


def generate_metadata(doc_dict: Dict) -> Dict:
    """ë©”íƒ€ë°ì´í„° ìë™ ìƒì„±"""
    
    pages = doc_dict.get("pages", [])
    
    metadata = {
        "total_pages": len(pages),
        "total_blocks": sum(len(p.get("blocks", [])) for p in pages),
        "total_paragraphs": sum(
            len(b.get("paragraphs", []))
            for p in pages
            for b in p.get("blocks", [])
        ),
        "detected_sections": list(set(
            p.get("detected_section", "unknown") for p in pages
        )),
        "has_currency": len(doc_dict.get("extracted_numbers", {}).get("currency", [])) > 0,
        "has_percentage": len(doc_dict.get("extracted_numbers", {}).get("percentage", [])) > 0,
        "language": "ko",
    }
    
    doc_dict["metadata"] = metadata
    return doc_dict


def _extract_block_text(block: Dict, full_text: str) -> str:
    """ë¸”ë¡ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    
    layout = block.get("layout", {})
    text_anchor = layout.get("textAnchor", {})
    segments = text_anchor.get("textSegments", [])
    
    texts = []
    for segment in segments:
        start = int(segment.get("startIndex", 0))
        end = int(segment.get("endIndex", 0))
        texts.append(full_text[start:end])
    
    return " ".join(texts).strip()


def merge_chunk_results(chunk_results: List[Dict], output_path: str) -> Dict:
    """ì—¬ëŸ¬ ì²­í¬ ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©"""
    
    if not chunk_results:
        raise ValueError("âŒ ë³‘í•©í•  ì²­í¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"\nğŸ”— {len(chunk_results)}ê°œ ì²­í¬ ê²°ê³¼ ë³‘í•© ì¤‘...")
    
    merged = {
        "text": "",
        "pages": [],
        "detected_sections": [],
        "extracted_numbers": {
            "currency": [],
            "percentage": [],
            "quantity": []
        },
        "metadata": {
            "total_chunks": len(chunk_results)
        }
    }
    
    page_offset = 0
    
    for chunk in chunk_results:
        merged["text"] += chunk.get("text", "")
        
        for page in chunk.get("pages", []):
            page["original_page_number"] = page_offset + page.get("pageNumber", 0)
            merged["pages"].append(page)
        
        page_offset += len(chunk.get("pages", []))
        
        for section in chunk.get("detected_sections", []):
            section["page"] += page_offset - len(chunk.get("pages", []))
            merged["detected_sections"].append(section)
        
        numbers = chunk.get("extracted_numbers", {})
        for num_type in ["currency", "percentage", "quantity"]:
            merged["extracted_numbers"][num_type].extend(numbers.get(num_type, []))
    
    merged["metadata"]["total_pages"] = len(merged["pages"])
    merged["metadata"]["total_blocks"] = sum(
        len(p.get("blocks", [])) for p in merged["pages"]
    )
    
    save_json(merged, output_path)
    print(f"âœ… ë³‘í•© ì™„ë£Œ: {output_path}\n")
    
    return merged
