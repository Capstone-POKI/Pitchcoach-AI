import os
import json
from typing import Dict, Any, Tuple
from pathlib import Path

import numpy as np
import librosa
from openai import OpenAI
from google import genai
from google.genai import types

BASE_DIR = Path(__file__).resolve().parents[2]
AUDIO_FILE = BASE_DIR / "data" / "input" / "sample_sound.m4a"
DECK_JSON_PATH = BASE_DIR / "data" / "output" / "asleep_irdeck.json"
PROMPT_PATH = Path(__file__).resolve().with_name("whisper_prompt.text")
SCENARIO = "ì°½ì—…ê²½ì§„ëŒ€íšŒ"

SCENARIO_CONFIG = {
    "VC ë°ëª¨ë°ì´": {
        "target_wpm": (150, 190),
        "importance": {"speed": 0.4, "intonation": 0.3, "clarity": 0.3},
    },
    "ì°½ì—…ê²½ì§„ëŒ€íšŒ": {
        "target_wpm": (130, 170),
        "importance": {"speed": 0.3, "intonation": 0.3, "clarity": 0.4},
    },
    "ì •ë¶€ì§€ì›Â·ì •ì±… IR": {
        "target_wpm": (110, 150),
        "importance": {"speed": 0.25, "intonation": 0.25, "clarity": 0.5},
    },
    "1ë¶„ ì—˜ë¦¬ë² ì´í„° í”¼ì¹˜": {
        "target_wpm": (160, 210),
        "importance": {"speed": 0.5, "intonation": 0.3, "clarity": 0.2},
    },
}

with open(PROMPT_PATH, "r", encoding="utf-8") as f:
    IR_PROMPT_TEMPLATE = f.read()

openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

gemini_client = genai.Client(
    vertexai=True,
    project=os.getenv("PROJECT_ID"),
    location=os.getenv("LOCATION"),
)


def load_deck_json(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def build_deck_context_text(deck_json: Dict[str, Any]) -> str:
    lines = []
    lines.append("[IR ë± ë¶„ì„ ìš”ì•½]")

    diagnosis = deck_json.get("diagnosis", {})
    missing_sections = diagnosis.get("missing_sections", [])
    if missing_sections:
        lines.append(f"- ë¹ ì§„ ì„¹ì…˜: {', '.join(missing_sections)}")

    slides = deck_json.get("slides", [])
    if slides:
        lines.append("\n[ìŠ¬ë¼ì´ë“œë³„ ìš”ì•½]")
        for slide in slides:
            page = slide.get("page_number")
            section = slide.get("section_type", "")
            contents = slide.get("contents", {})
            summary = contents.get("summary") or contents.get("full_text", "")[:80]
            voice_guide = slide.get("voice_guide", {})
            est_sec = voice_guide.get("estimated_duration_sec")

            line = f"- p.{page} ({section}): {summary}"
            if est_sec:
                line += f" / ê¶Œì¥ ë°œí™” ì‹œê°„: {est_sec}ì´ˆ"
            lines.append(line)

    return "\n".join(lines)


def transcribe_audio(path: Path) -> str:
    with path.open("rb") as audio_file:
        result = openai_client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
        )
    return result.text


def extract_audio_features(path: Path) -> Tuple[float, Dict[str, float]]:
    y, sr = librosa.load(str(path), sr=None)
    duration = librosa.get_duration(y=y, sr=sr)

    energy = y ** 2
    energy_std = float(np.std(energy))

    thresh = np.percentile(energy, 20)
    silence_ratio = float(np.mean(energy < thresh))

    pitch_mean = 0.0
    pitch_std = 0.0
    pitch_range = 0.0

    try:
        f0, _, _ = librosa.pyin(
            y,
            fmin=librosa.note_to_hz("C2"),
            fmax=librosa.note_to_hz("C7"),
            sr=sr,
        )
        f0_clean = f0[~np.isnan(f0)]
        if len(f0_clean) > 0:
            pitch_mean = float(np.mean(f0_clean))
            pitch_std = float(np.std(f0_clean))
            pitch_range = float(np.max(f0_clean) - np.min(f0_clean))
    except Exception:
        pass

    return duration, {
        "duration": duration,
        "energy_std": energy_std,
        "pitch_mean": pitch_mean,
        "pitch_std": pitch_std,
        "pitch_range": pitch_range,
        "silence_ratio": silence_ratio,
    }


def calc_wpm(transcript: str, duration_sec: float) -> float:
    words = transcript.strip().split()
    if duration_sec <= 0 or not words:
        return 0.0
    minutes = duration_sec / 60.0
    return round(len(words) / minutes, 1)


def analyze_with_gemini(
    transcript_text: str,
    scenario: str,
    wpm: float,
    features: Dict[str, float],
    deck_json: Dict[str, Any],
) -> str:
    deck_ctx = build_deck_context_text(deck_json)
    scenario_cfg = SCENARIO_CONFIG.get(scenario, SCENARIO_CONFIG["VC ë°ëª¨ë°ì´"])
    target_low, target_high = scenario_cfg["target_wpm"]
    imp = scenario_cfg["importance"]

    audio_ctx = f"""
[ë°œí‘œ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •]

- í˜„ì¬ ë¶„ì„ ëŒ€ìƒ ë°œí‘œ ìƒí™©: {scenario}
- ì´ ìƒí™©ì—ì„œ ê¶Œì¥ ë§í•˜ê¸° ì†ë„ ë²”ìœ„: ì•½ {target_low} ~ {target_high} WPM
- ì´ ìƒí™©ì—ì„œì˜ í‰ê°€ ì¤‘ìš”ë„ ë¹„ì¤‘:
  Â· ì†ë„(speed): {int(imp["speed"] * 100)}%
  Â· ì–µì–‘Â·ê°•ì¡°(intonation): {int(imp["intonation"] * 100)}%
  Â· ëª…ë£Œì„±(clarity): {int(imp["clarity"] * 100)}%

[ìŒì„± ë¶„ì„ ìš”ì•½]

- ì‹¤ì œ ì¸¡ì • ë§í•˜ê¸° ì†ë„(WPM): {wpm}
- ì „ì²´ ìŒì„± ê¸¸ì´(ì´ˆ): {features.get("duration", 0):.1f}
- í”¼ì¹˜ í‰ê· (pitch_mean, Hz): {features.get("pitch_mean", 0):.2f}
- í”¼ì¹˜ í‘œì¤€í¸ì°¨(pitch_std): {features.get("pitch_std", 0):.2f}
- í”¼ì¹˜ ë²”ìœ„(pitch_range): {features.get("pitch_range", 0):.2f}
- ì—ë„ˆì§€ í‘œì¤€í¸ì°¨(energy_std): {features.get("energy_std", 0):.4f}
- ì¹¨ë¬µ ë¹„ìœ¨(silence_ratio): {features.get("silence_ratio", 0):.3f}

ìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬
- 'ë§í•˜ê¸°_ì†ë„_WPM' í•„ë“œì—ëŠ” ì‹¤ì œ ì¸¡ì •ê°’ì¸ {wpm}ì„ ë„£ìœ¼ì„¸ìš”.
- 'ì–µì–‘_ê°•ì¡°_ì•ˆì •ì„±'ì€ ì£¼ë¡œ í”¼ì¹˜ í‰ê· /í‘œì¤€í¸ì°¨/ë²”ìœ„ì™€ ì—ë„ˆì§€ ë³€ë™ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ,
- 'ë¬¸ì¥_ëª…ë£Œì„±'ê³¼ 'ë¶ˆí•„ìš”í•œ_ë§ë²„ë¦‡'ì€ ì¹¨ë¬µ ë¹„ìœ¨ê³¼ ì†ë„(WPM)ë¥¼ ì°¸ê³ í•˜ì—¬,
- í•´ë‹¹ ì‹œë‚˜ë¦¬ì˜¤ì˜ ê¶Œì¥ ì†ë„ ë²”ìœ„ì™€ ì¤‘ìš”ë„(weight)ë¥¼ ê³ ë ¤í•´
êµ¬ì²´ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

ìµœì¢… ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì§€ì •ëœ JSON êµ¬ì¡°ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
"""

    prompt_prefix = deck_ctx + "\n\n" + audio_ctx + "\n\n"
    final_prompt = prompt_prefix + IR_PROMPT_TEMPLATE.replace('{{$json["text"]}}', transcript_text)

    response = gemini_client.models.generate_content(
        model="gemini-2.0-flash",
        contents=final_prompt,
        config=types.GenerateContentConfig(
            response_mime_type="application/json",
            temperature=0.2,
        ),
    )
    return response.text


def main():
    deck_json = load_deck_json(DECK_JSON_PATH)

    print("ğŸ§ Whisperë¡œ ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")
    transcript_text = transcribe_audio(AUDIO_FILE)

    print("\nğŸ¼ librosaë¡œ ìŒì„± íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    duration_sec, features = extract_audio_features(AUDIO_FILE)
    wpm = calc_wpm(transcript_text, duration_sec)

    print("\nğŸ§  Geminië¡œ IR ë°œí‘œ ë¶„ì„ ì¤‘...")
    json_result = analyze_with_gemini(
        transcript_text=transcript_text,
        scenario=SCENARIO,
        wpm=wpm,
        features=features,
        deck_json=deck_json,
    )

    print("\n--- Gemini JSON ê²°ê³¼ ---")
    print(json_result)


if __name__ == "__main__":
    main()